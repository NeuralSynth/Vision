# React + Vite

This template provides a minimal setup to get React working in Vite with HMR and some ESLint rules.

Currently, two official plugins are available:

- [@vitejs/plugin-react](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react/README.md) uses [Babel](https://babeljs.io/) for Fast Refresh
- [@vitejs/plugin-react-swc](https://github.com/vitejs/vite-plugin-react-swc) uses [SWC](https://swc.rs/) for Fast Refresh

Vision Assistance App
ğŸ“ Overview
The Vision Assistance App is designed to help individuals who are completely or heavily partially blind navigate the real world independently. It utilizes AI-powered object detection, haptic feedback, and emergency features to enhance accessibility and safety.

âœ¨ Features
ğŸ” Object Detection & Distance Feedback
Uses the mobile deviceâ€™s camera to detect objects in real-time.
Divides the field of view into 9 quadrants (3x3 grid) to indicate object location.
Estimates object distance using vibration motors (intensity-based feedback).
Provides haptic feedback for better interaction.
Recognizes household items and human faces accurately.
ğŸ› Interactive Buttons & Controls
9 on-screen buttons mapped to the 9 quadrants:
ğŸ“ Describe Object in Detail â€“ Short description first, detailed info on press.
ğŸ“ Call Emergency Contact â€“ Quick dial to a preset number (family, caretaker, etc.). 3-9. ğŸ› Additional customizable buttons for:
Voice commands ğŸ™.
Saving object details for later reference.
Adjusting vibration intensity âš™.
Toggling features on/off ğŸ“´.
Physical button shortcuts using volume and power button combinations.
ğŸš¨ Accident Detection & Emergency Alerts
GPS functionality for navigation and location tracking.
Fall detection that automatically sends emergency alerts ğŸ“¡.
Health info storage at startup for medical emergencies ğŸ¥.
Auto emergency message ğŸ“© with location and medical info sent to hospitals and family members.
ğŸ”§ Additional Features
Voice assistance ğŸ™ for optional audio cues.
Configurable grid system (3x3 or 2x4) ğŸ“ for different navigation needs.
AI-powered object recognition ğŸ¤– for improved accuracy.
Offline functionality âš¡ for essential features.
Customizable vibration patterns for different object types.
ğŸš€ How It Works
Open the app ğŸ“².
Point the camera at your surroundings ğŸ¥.
Receive real-time object detection feedback with vibration and haptic cues ğŸ“³.
Use on-screen buttons for additional assistance ğŸ›.
Enable GPS navigation or fall detection alerts for added safety ğŸ—º.
ğŸ›  Tech Stack
Language: Java/Kotlin (Android) or Swift (iOS)
Machine Learning: TensorFlow Lite / OpenCV / YOLO
Haptic Feedback: Android Vibration API, iOS Core Haptics
GPS & Emergency Alerts: Google Maps API, Twilio API
ğŸ“Œ Future Enhancements
ğŸŒ Multi-language support.
ğŸµ Dynamic sound cues for navigation.
ğŸ¦¾ Integration with smart wearables (e.g., neosapien with camera, smart glasses).
ğŸ—‚ Cloud-based object recognition for improved AI training.
ğŸ— Installation & Setup
Clone the repo:
git clone https://github.com/yourusername/Vision-Assistance-App.git
Open the project in Android Studio/Xcode.
Build and install on a test device.
ğŸ“œ License
This project is licensed under the MIT License.
