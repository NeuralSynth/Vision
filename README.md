# Vision Assistance App

## Overview
The **Vision Assistance App** is designed to empower individuals who are completely or heavily partially blind by helping them navigate the real world independently. Leveraging AI-powered object detection, haptic feedback, and emergency features, the app enhances accessibility and safety.

---
## âœ¨ Key Features

### ğŸ” Object Detection & Distance Feedback
- Uses the mobile deviceâ€™s camera to detect objects in real-time.
- Divides the field of view into a **9-quadrant grid (3x3)** for precise object localization.
- Estimates object distance and provides **intensity-based haptic feedback**.
- Recognizes common household items and human faces accurately.

### ğŸ› Interactive Controls
- **On-Screen Buttons:**
  - **Describe Object in Detail:** Short description on tap, detailed info on long press.
  - **Call Emergency Contact:** Quick dial to a preset contact (family, caretaker, etc.).
  - **Customizable Buttons:**
    - Voice commands ğŸ™
    - Saving object details for later reference ğŸ“
    - Adjusting vibration intensity âš™
    - Toggling features on/off ğŸ“´
  - **Physical Button Shortcuts:** Volume and power button combinations for quick actions.

### ğŸš¨ Emergency Features
- **GPS Functionality:** Navigation and location tracking ğŸ—º
- **Fall Detection:** Auto emergency alert activation ğŸ“¡
- **Medical Information Storage:** Quick access to health data for emergencies ğŸ¥
- **Automated Emergency Messages:** Sends location and medical info to hospitals and family members ğŸ“©

### ğŸ”§ Additional Functionalities
- **Voice Assistance:** Audio cues for enhanced interaction ğŸ™
- **Customizable Grid Layouts:** Choose between **3x3** or **2x4** grid systems ğŸ“
- **AI-Powered Object Recognition:** Powered by **TensorFlow Lite, OpenCV, or YOLO** ğŸ¤–
- **Offline Functionality:** Essential features work without internet âš¡
- **Personalized Vibration Patterns:** Different haptic responses for various object types

---
## ğŸš€ How It Works
1. **Launch the app** ğŸ“²
2. **Point the camera at your surroundings** ğŸ¥
3. **Receive real-time feedback** via haptic cues ğŸ“³
4. **Use on-screen or physical controls** for additional assistance ğŸ›
5. **Enable GPS navigation or emergency alerts** for added security ğŸ—º

---
## ğŸ›  Tech Stack
- **Mobile Platforms:** Java/Kotlin (Android) | Swift (iOS)
- **Machine Learning:** TensorFlow Lite / OpenCV / YOLO
- **Haptic Feedback:** Android Vibration API, iOS Core Haptics
- **GPS & Emergency Alerts:** Google Maps API, Twilio API

---
## ğŸ“Œ Future Enhancements
- ğŸŒ **Multi-language support** for global accessibility
- ğŸµ **Dynamic sound cues** to aid navigation
- ğŸ¦¾ **Integration with smart wearables** (e.g., smart glasses, AI-powered assistants)
- ğŸ—‚ **Cloud-based AI training** for improved object recognition

---
## ğŸ— Installation & Setup
### Clone the repository:
```sh
git clone https://github.com/yourusername/Vision-Assistance-App.git
```
### Setup Instructions:
1. Open the project in **Android Studio/Xcode**.
2. Build and install the app on a test device.

---
## ğŸ“œ License
This project is licensed under the **MIT License**.

